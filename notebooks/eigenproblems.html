
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>29. Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond &#8212; Elementary Numerical Analysis (with Python)</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="30. Least-squares Fitting to Data" href="least-squares-fitting-python.html" />
    <link rel="prev" title="28. Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction" href="minimization-1D.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/UNC_BearMascot.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Elementary Numerical Analysis (with Python)</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   Elementary Numerical Analysis with Python
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../plans-for-expansion-improvement.html">
   Full Disclosure: Things I Plan to do to Expand and Improve This Book
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Numerical Analysis
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-by-interval-halving-python.html">
   1. Root Finding by Interval Halving (Bisection)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fixed-point-iteration-python.html">
   2. Solving Equations by Fixed Point Iteration (of Contraction Mappings)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-python.html">
   3. Newton’s Method for Solving Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="taylors-theorem.html">
   4. Taylor’s Theorem and the Accuracy of Linearization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="error-measures-convergence-rates.html">
   5. Measures of Error and Order of Convergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-convergence-rate.html">
   6. The Convergence Rate of Newton’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-without-derivatives-python.html">
   7. Root-finding Without Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-1-row-reduction-python.html">
   8. Simultaneous Linear Equations, Part 1: Row Reduction/Gaussian Elimination
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machine-numbers-rounding-error-and-error-propagation-python.html">
   9. Machine Numbers, Rounding Error and Error Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-2-pivoting-python.html">
   10. Simultaneous Linear Equations, Part 2: Partial Pivoting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-3-lu-factorization-python.html">
   11. Simultaneous Linear Equations, Part 3: Solving
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
   with LU factorization,
   <span class="math notranslate nohighlight">
    \(A = L U\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-4-plu-factorization-python.html">
   12. Simultaneous Linear Equations, Part 4: Solving
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
   With Both Pivoting and LU factorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="polynomial-collocation%2Bapproximation-python.html">
   13. Polynomial Collocation (Interpolation/Extrapolation) and Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="polynomial-collocation-error-formulas-python.html">
   14. Error Formulas for Polynomial Collocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="derivatives-and-the-method-of-undetermined-coefficents.html">
   15. Approximating Derivatives by the Method of Undetermined Coefficients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="richardson-extrapolation.html">
   16. Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-1-building-blocks-python.html">
   17. Definite Integrals, Part 1: The Building Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-2-composite-rules.html">
   18. Definite Integrals, Part 2: The Composite Trapezoid and Midpoint Rules
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-3-simpson-richardson.html">
   19. Definite Integrals, Part 3: The (Composite) Simpson’s Rule and Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-4-romberg-integration.html">
   20. Definite Integrals, Part 4: Romberg Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-1-basics-and-Euler-python.html">
   21. Initial Value Problems for Ordinary Differential Equations, Part 1: Basic Concepts and Euler’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-2-Runge-Kutta-python.html">
   22. Initial Value Problems for ODEs, Part 2: Runge-Kutta Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-3-error-results-one-step-methods.html">
   23. Initial Value Problems for Ordinary Differential Equations, Part 3: Global Error Bounds for One Step Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-4-system-higher-order-equations.html">
   24. Initial Value Problems for Ordinary Differential Equations, Part 4: Systems of ODEs and Higher Order ODEs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-5-error-control-python.html">
   25. Initial Value Problems for Ordinary Differential Equations, Part 5: Error Control and Variable Step Sizes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-5-error-bounds-condition-numbers-python.html">
   26. Simultaneous Linear Equations, Part 5: Error bounds for linear algebra, condition numbers, matrix norms, etc.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-6-iterative-methods.html">
   27. Simultaneous Linear Equations, Part 6: Iterative Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="minimization-1D.html">
   28. Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   29. Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="least-squares-fitting-python.html">
   30. Least-squares Fitting to Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="least-squares-fitting-appendix-geometrical-approach.html">
   31. Least-squares Fitting to Data: The Geometrical Approach
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../future_topics.html">
   32. Some Future Topics
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preliminary Versions of Some Possible Future Sections
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-for-systems-introduction.html">
   1. Solving Nonlinear Systems of Equations by generalizations of Newton’s Method — a Brief Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-7-tridiagonal-banded-and-SDD-matrices-preliminary.html">
   2. Simultaneous Linear Equations, Part 7: Faster Methods for Solving
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
   for Tridiagonal and Banded matrices, and Strict Diagonal Dominance
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-by-interval-halving-exercises-python.html">
   Exercises on the Bisection Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fixed-point-iteration-exercises.html">
   Exercises on Fixed Point Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="error-measures-convergence-rates-exercises.html">
   Exercises on Error Measures and Convergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-exercises.html">
   Exercises on Newton’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-without-derivatives-exercises.html">
   Exercises on Root-finding Without Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machine-numbers-rounding-error-and-error-propagation-exercises.html">
   Exercises on Machine Numbers, Rounding Error and Error Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-exercises.html">
   Exercises on Solving Simultaneous Linear Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="derivatives-and-the-method-of-undetermined-coefficents-exercises.html">
   Exercises on Approximating Derivatives, the Method of Undetermined Coefficients and Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-exercises.html">
   Exercises on Initial Value Problems for Ordinary Differential Equations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="assignment1.html">
   MATH 375 Assignment 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment2.html">
   MATH 375 Assignment 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment3.html">
   MATH 375 Assignment 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root_finding.html">
   Module root_finding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment4.html">
   MATH 375 Assignment 4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment5.html">
   MATH 375 Assignment 5
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Project on Numerical Calculus
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-1-and-2-centered-difference-derivative-approximation-and-richardson.html">
   Centered Difference Approximation of the Derivative
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-1-and-2-centered-difference-derivative-approximation-and-richardson.html#improving-on-the-centered-difference-approximation-with-richardson-extrapolation">
   Improving on the Centered Difference Approximation with Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-3-composite-trapezoid-rule.html">
   The Composite Trapezoid Rule (and Composite Midpoint Rule)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-4-recursive-trapezoid-rule.html">
   The Recursive Trapezoid Rule, with error control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-5-and-6-modified-trapezoid-method.html">
   The Modified Trapezoid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-7-romberg-method.html">
   The Romberg Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/test-cases-differentiation.html">
   Test Cases for Differentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/test-cases-integration.html">
   Test Cases for Integration
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Some Final Project Possibilities
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html">
   Some Final Project Possibilites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#minimizing-functions-of-one-and-several-variables">
   Minimizing Functions of One and Several Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#root-finding-by-repeated-inverse-quadratic-approximation-with-bracketing">
   Root-finding by Repeated Inverse Quadratic Approximation with Bracketing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#a-more-reliable-secant-method">
   A More Reliable Secant Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#computing-eigenvalues-and-eigenvectors">
   Computing Eigenvalues and Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#iterative-methods-for-solving-simultaneous-linear-equations-ax-b">
   Iterative Methods for Solving Simultaneous Linear Equations,
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#solving-simultaneous-nonlinear-equations">
   Solving Simultaneous Nonlinear Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#fitting-smooth-piecewise-cubic-functions-to-data">
   Fitting Smooth Piecewise Cubic Functions to Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#least-squares-fitting-to-data-and-functions">
   Least-Squares Fitting to Data and Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#boundary-value-problems-for-differential-equations">
   Boundary Value Problems for Differential Equations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Python and Jupyter Notebook Review (with Numpy and Matplotlib)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/introduction.html">
   1. Introduction to
   <em>
    Python Preview
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/getting-python-software-for-scientific-computing.html">
   2. Getting Python Software for Scientific Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/suggestions-on-python-and-notebook-usage.html">
   3. Suggestions and Notes on Python and Jupyter Notebook Usage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/python-variables-lists-tuples-numpy-arrays.html">
   4. Python Variables, Including Lists and Tuples, and Arrays from Package Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/functions.html">
   5. Defining and Using Python Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/decisions-with-if-else-elif.html">
   6. Decision Making With if, else, and elif
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/iteration-with-for.html">
   7. Iteration with
   <code class="docutils literal notranslate">
    <span class="pre">
     for
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/iteration-with-while.html">
   8. Iteration with
   <code class="docutils literal notranslate">
    <span class="pre">
     while
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/code-files-modules-IDEs.html">
   9. Code Files, Modules, and an Integrated Development Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/recursion.html">
   10. Recursion (vs iteration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/graphing-with-matplotlib.html">
   11. Plotting Graphs with Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/array-operations-and-linear-algebra.html">
   12. Numpy Array Operations and Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/scipy-tools-for-linear-algebra.html">
   13. Package Scipy and More Tools for Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/summation-and-integration.html">
   14. Summation and Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/random-numbers-and-simulation.html">
   15. Random Numbers, Histograms, and a Simulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/formatted-output-and-some-text-string-manipulation.html">
   16. Formatted Output and Some Text String Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/classes-objects-attributes-methods.html">
   17. Classes, Objects, Attributes, Methods: Very Basic Object-Oriented Programming in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/exception-handling.html">
   18. Exceptions and Exception Handling
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="numerical_methods_module.html">
   Notebook for generating the module
   <code class="docutils literal notranslate">
    <span class="pre">
     numerical_methods_module
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-algebra-with-0-based-indexing-and-semiopen-intervals.html">
   Linear algebra algorithms using 0-based indexing and semi-open intervals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sample-project-newtons-method-python-draft.html">
   Numerical Analysis Sample Project on Newtons’s Method
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/eigenproblems.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/eigenproblems.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-power-method">
   29.1. The Power Method
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-basic-algorithm-for-the-power-method">
   29.2. A basic algorithm for the Power Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     29.2.1. Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#refinement-deciding-the-iteration-count">
     29.2.2. Refinement: deciding the iteration count
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2">
     29.2.3. Exercise 2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-inverse-power-method">
   29.3. The Inverse Power Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#basic-algorithm-for-the-inverse-power-method">
     29.3.1. Basic algorithm for the Inverse Power Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-3">
     29.3.2. Exercise 3
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#getting-other-eigenvalues-with-the-shifted-inverse-power-method">
   29.4. Getting other eigenvalues with the Shifted Inverse Power Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-4">
     29.4.1. Exercise 4
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc">
   29.5. Further topics: getting all the eigenvalues with the QR Method, etc.
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="computing-eigenvalues-and-eigenvectors-the-power-method-and-a-bit-beyond">
<h1><span class="section-number">29. </span>Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond<a class="headerlink" href="#computing-eigenvalues-and-eigenvectors-the-power-method-and-a-bit-beyond" title="Permalink to this headline">¶</a></h1>
<p><strong>Version of Friday April 16, 2021</strong>, making a few corrections after class and adding a reference for the
<a class="reference external" href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Gershgorin circle theorem</a>.</p>
<p><strong>References:</strong></p>
<ul class="simple">
<li><p>Section 12.1 <em>Power Iteration Methods</em> of <a class="reference external" href="../references.html#Sauer">Sauer</a></p></li>
<li><p>Section 7.2 <em>Eigenvalues and Eigenvectors</em> of <a class="reference external" href="../references.html#Burden-Faires">Burden&amp;Faires</a>.</p></li>
<li><p>Chapter 8, <em>More on Linear Equations</em> of <a class="reference external" href="../references.html#Chenney-Kincaid">Chenney&amp;Kincaid</a>,
in particular Section 3 <em>Power Method</em>, and also Section 2 <em>Eigenvalues and Eigenvectors</em> as background reading.</p></li>
</ul>
<p>The <em>eigenproblem</em> for a square <span class="math notranslate nohighlight">\(n \times n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> is to compute some or all non-trivial solutions of</p>
<div class="math notranslate nohighlight">
\[A \vec{v} = \lambda \vec{v}.\]</div>
<p>(By non-trivial, I mean to exclude <span class="math notranslate nohighlight">\(\vec{v} = 0\)</span>, which gives a solution for any <span class="math notranslate nohighlight">\(\lambda\)</span>.)
That is, to compute
the eigenvalues <span class="math notranslate nohighlight">\(\lambda\)</span> (of which generically there are <span class="math notranslate nohighlight">\(n\)</span>, but sometimes less)
and the eigenvectors <span class="math notranslate nohighlight">\(\vec{v}\)</span> corresponding to each.</p>
<p>With eigenproblems, and particularly those arising from differential equations, one often needs only the few smallest and/or largest eigenvalues. For these, the <em>power method</em> described next can be adapted, leading to the <em>shifted inverse power method</em>.</p>
<p>Here we often restict our attention to the case of a real <em>symmetric</em> matrix (<span class="math notranslate nohighlight">\(A^T = A\)</span>, or <span class="math notranslate nohighlight">\(A_{ij} = A_{ji}\)</span>), or a Hermitian matrix (<span class="math notranslate nohighlight">\(A^T = A^*\)</span>), for which many things are a bit simpler:</p>
<ul class="simple">
<li><p>all eigenvalues are real,</p></li>
<li><p>for symmetric matrices, all eigenvectors are also real,</p></li>
<li><p>there is a complete set of orthogonal eigenvectors <span class="math notranslate nohighlight">\(\vec{v}_k\)</span>, <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span> that form a basis for all vectors,
and so on.</p></li>
</ul>
<p>However, the methods described here can be used more generally, or can be made to work with minor adjustments.</p>
<p>The eigenvalue are roots of the characteristic polynomial, <span class="math notranslate nohighlight">\(\det(A - \lambda I)\)</span>;
repeated roots are possible, and they will all be named, so there are always values <span class="math notranslate nohighlight">\(\lambda_i\)</span>, <span class="math notranslate nohighlight">\(1 \leq i \leq n\)</span>.
Here, these eigenvalues will be enumerated in decreasing order of magnitude:</p>
<div class="math notranslate nohighlight">
\[|\lambda_1| \geq |\lambda_2| \cdots \geq |\lambda_n|.\]</div>
<p>Generically, all the magnitudes are different, which makes things works more easily,
so that will sometimes be assumed while developing the intuition of the method.</p>
<div class="section" id="the-power-method">
<h2><span class="section-number">29.1. </span>The Power Method<a class="headerlink" href="#the-power-method" title="Permalink to this headline">¶</a></h2>
<p>The basic tool is the <em>Power Method</em>, which will usually <em>but not always</em> succeed in computing the eigenvalue of largest magnitude, <span class="math notranslate nohighlight">\(\lambda_1\)</span>, and <strong>a</strong> corresponding eigenvector <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>.
Its success mainly involves assuming there being a unique largest eigenvalue: <span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_i\)</span> for <span class="math notranslate nohighlight">\(i&gt;1\)</span>.</p>
<p>In its simplest form, one starts with a unit-length vector <span class="math notranslate nohighlight">\(\vec{x}^{\,0}\)</span>,
so <span class="math notranslate nohighlight">\(\|\vec{x}^{\,0}\| = 1\)</span>, constructs the successive multiples
<span class="math notranslate nohighlight">\(\vec{y}^{\,k} = A^k \vec{x}^{\,0}\)</span>
by successive multiplications, and rescales at each stage to the unit vectors
<span class="math notranslate nohighlight">\(\vec{x}^{\,k} = \vec{y}^{\,k}/\|\vec{y}^{\,k}\|\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} = A \vec{x}^{\,k}\)</span>, so that once <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> is approximately an eigenvector for eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} \approx \lambda \vec{x}^{\,k}\)</span>, leading to the eigenvalue approximation</p>
<div class="math notranslate nohighlight">
\[r^{(k)} := \langle\vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle
\approx \langle\lambda \vec{x}^{\,k}, \vec{x}^{\,k}\rangle \approx \lambda\]</div>
<p><strong>Note:</strong> Here and below, I use <span class="math notranslate nohighlight">\(\langle \vec{a}, \vec{b}\rangle\)</span> to denote the inner product (a.k.a. <em>dot product</em>) of two vectors; with Numpy arrays this is given by <code class="docutils literal notranslate"><span class="pre">a.dot(b)</span></code>.</p>
</div>
<div class="section" id="a-basic-algorithm-for-the-power-method">
<h2><span class="section-number">29.2. </span>A basic algorithm for the Power Method<a class="headerlink" href="#a-basic-algorithm-for-the-power-method" title="Permalink to this headline">¶</a></h2>
<p><em>Choose</em> initial vector <span class="math notranslate nohighlight">\(\vec{y}_0\)</span>, maybe with a random number generator.</p>
<p>Normalize to <span class="math notranslate nohighlight">\(\vec{x}^{\,0} = \vec{y}^{\,0}/\|\vec{y}^{\,0}\|\)</span>.</p>
<p>for <span class="math notranslate nohighlight">\(k\)</span> from 0 to <span class="math notranslate nohighlight">\(k_{max}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{y}^{\,k+1} = A \vec{x}^{\,k}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(r^{(k)} = \langle \vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{x}^{\,k+1} = \vec{y}^{\,k+1}/\|\vec{y}^{\,k+1}\|\)</span>
<br>
end for</p>
<p>The final values of <span class="math notranslate nohighlight">\(r^{(k)}\)</span> and <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> approximate <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_1\)</span> respectively.</p>
<div class="section" id="exercise-1">
<h3><span class="section-number">29.2.1. </span>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>Implement this algorithm and test it on the real, symmetric matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \left[ \begin{array}{ccc} 3 &amp; 1 &amp; 1 \\ 1 &amp; 8 &amp; 1 \\ 1 &amp; 1 &amp; 4 \end{array} \right]
\end{split}\]</div>
<p>This all real eigenvalues, all within <span class="math notranslate nohighlight">\(2\)</span> of the diagonal elements (this claim should be explained as part of the project write-up), so start with it.</p>
<p>As a debugging strategy, you could replace all those off-diagonal ones by a small value <span class="math notranslate nohighlight">\(\delta\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_\delta = \left[ \begin{array}{ccc} 3 &amp; \delta &amp; \delta \\ \delta &amp; 8 &amp; \delta \\ \delta &amp; \delta &amp; 4 \end{array} \right]
\end{split}\]</div>
<p>Then the <a class="reference external" href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Gershgorin circle theorem</a> ensures that each eigenvalue is within <span class="math notranslate nohighlight">\(2\delta\)</span> of an entry on the main diagonal.
Furthermore, if <span class="math notranslate nohighlight">\(\delta\)</span> is small enough that the circles of radius <span class="math notranslate nohighlight">\(2\delta\)</span> centered on the diagonal elements do not overlap, then there is one eigenvalue in each circle.</p>
<p>You could even start with <span class="math notranslate nohighlight">\(\delta=0\)</span>, for which yuo know exactly the eigenvalues: they are the diagonal elements.</p>
<p>Here and below you could check your work with Numpy, using function <code class="docutils literal notranslate"><span class="pre">numpy.linalg.eig(A)</span></code>.</p>
<p>However, thst is almost cheating, so note that there is also a backward error check:
see how small <span class="math notranslate nohighlight">\(\|A v - \lambda v\|/\| v \|\)</span> is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">la</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">la</span><span class="o">.</span><span class="n">eig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">delta</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">delta</span><span class="p">],[</span><span class="n">delta</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">delta</span><span class="p">],[</span><span class="n">delta</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="p">[</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="p">]</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;With </span><span class="si">{</span><span class="n">delta</span><span class="si">=}</span><span class="s2">, so that A is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">lambda_0</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">lambda_1</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">lambda_2</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="c1"># The eigenvectors are the columns of the output array `eigenvectors`, so:</span>
<span class="n">v_0</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>   <span class="c1"># Lists print more nicely than arrays!</span>
<span class="n">v_1</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">v_2</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">eigenvectors</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The eigenvalues are </span><span class="si">{</span><span class="n">eigenvalues</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;and the eigenvalue-eigenvector pairs are&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lambda_0</span><span class="si">=}</span><span class="s2">, </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">v_0</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lambda_1</span><span class="si">=}</span><span class="s2">, </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">v_1</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lambda_2</span><span class="si">=}</span><span class="s2">, </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">v_2</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>With delta=0.01, so that A is
[[3.   0.01 0.01]
 [0.01 8.   0.01]
 [0.01 0.01 4.  ]]
The eigenvalues are [2.99988041 8.0000451  4.00007449]
and the eigenvalue-eigenvector pairs are
lambda_0=2.999880409987065, 	 v_0=[-0.9999482535404536, 0.0019798921714429085, 0.009978490285907764]
lambda_1=8.000045099761195, 	 v_1=[0.0020049815629866285, 0.999994852570506, 0.0025049713419310364]
lambda_2=4.000074490251736, 	 v_2=[0.00997347934918302, -0.0025248484075827684, 0.9999470760246215]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="refinement-deciding-the-iteration-count">
<h3><span class="section-number">29.2.2. </span>Refinement: deciding the iteration count<a class="headerlink" href="#refinement-deciding-the-iteration-count" title="Permalink to this headline">¶</a></h3>
<p>Some details are omitted above; above all, how to decide the number of iterations.</p>
<p>One approach is to use the fact that an eigenvector-eigenvalue pair satisfies
<span class="math notranslate nohighlight">\(A \vec{v} - \lambda \vec{v} = 0\)</span>, so the “residual norm”</p>
<div class="math notranslate nohighlight">
\[
\frac{\|A \vec{x}^{\,k} - r^{(k)} \vec{x}^{\,k}\|}{\|\vec{x}^{\,k}\|},
= \|\vec{y}^{\,k+1} - r^{(k)} \vec{x}^{\,k}\| \text{ since $\|\vec{x}^{\,k}\| = 1$}
\]</div>
<p>is a measure of “relative backward error”.</p>
<p>Thus one could repace the above <code class="docutils literal notranslate"><span class="pre">for</span></code> loop by a <code class="docutils literal notranslate"><span class="pre">while</span></code> loop based on a condition like stopping when</p>
<div class="math notranslate nohighlight">
\[\|\vec{y}^{\,k+1} - r^{(k)} \vec{x}^{\,k}\| \leq \epsilon.\]</div>
<p>Alternatively, keep the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, but exit early (with <code class="docutils literal notranslate"><span class="pre">break</span></code>) if this condition is met.</p>
<p>I generally recommend this <code class="docutils literal notranslate"><span class="pre">for-if-break</span></code> form for implementing iterative methods, because it makes avoidance of infinite loops simpler, and avoids the common <code class="docutils literal notranslate"><span class="pre">while</span></code> loop issue that you do not yet have an error estimate when the loop starts.</p>
</div>
<div class="section" id="exercise-2">
<h3><span class="section-number">29.2.3. </span>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">¶</a></h3>
<p>Modify your code from Exercise 1 to implement this accuracy control.</p>
</div>
</div>
<div class="section" id="the-inverse-power-method">
<h2><span class="section-number">29.3. </span>The Inverse Power Method<a class="headerlink" href="#the-inverse-power-method" title="Permalink to this headline">¶</a></h2>
<p>The next step is to note that if <span class="math notranslate nohighlight">\(A\)</span> is nonsingular, its inverse <span class="math notranslate nohighlight">\(B = A^{-1}\)</span> has the same eigenvectors, but with eigenvalues <span class="math notranslate nohighlight">\(\mu_i = 1/\lambda_i\)</span>.</p>
<p>Thus we can apply the power method to <span class="math notranslate nohighlight">\(B\)</span> in order to compute its largest eigenvalue, which is <span class="math notranslate nohighlight">\(\mu_n = 1/\lambda_n\)</span>, along with the corresponding eigenvector <span class="math notranslate nohighlight">\(\vec{v}_n\)</span>.</p>
<p>The main change to the above is that</p>
<div class="math notranslate nohighlight">
\[\vec{y}^{\,k+1} = A^{-1} \vec{x}_{k}.\]</div>
<p>However, as usual one can (and should) avoid actually computing the inverse.
Instead, express the above as the sysem of equations.</p>
<div class="math notranslate nohighlight">
\[A \vec{y}^{\,k+1} = \vec{x}_{k}.\]</div>
<p>Here is an important case where the LU factorization method can speed things up greatly: a single LU factorization is needed, after which for each <span class="math notranslate nohighlight">\(k\)</span> one only has to do the far quicker forward and backward substitution steps: <span class="math notranslate nohighlight">\(O(n^2)\)</span> cost for each iteration instead of <span class="math notranslate nohighlight">\(O(n^3/3)\)</span>.</p>
<div class="section" id="basic-algorithm-for-the-inverse-power-method">
<h3><span class="section-number">29.3.1. </span>Basic algorithm for the Inverse Power Method<a class="headerlink" href="#basic-algorithm-for-the-inverse-power-method" title="Permalink to this headline">¶</a></h3>
<p><em>Choose</em> initial vector <span class="math notranslate nohighlight">\(\vec{y}_0\)</span>, maybe with a random number generator.</p>
<p>Normalize to <span class="math notranslate nohighlight">\(\vec{x}^{\,0} = \vec{y}^{\,0}/\|\vec{y}^{\,0}\|\)</span>.</p>
<p>Compute an LU factorization <span class="math notranslate nohighlight">\(L U = A\)</span>.</p>
<p>for <span class="math notranslate nohighlight">\(k\)</span> from 0 to <span class="math notranslate nohighlight">\(k_{max}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> Solve <span class="math notranslate nohighlight">\(L \vec{z}^{\,k+1} = \vec{x}^{\,k}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> Solve <span class="math notranslate nohighlight">\(U \vec{y}^{\,k+1} = \vec{z}^{\,k+1}\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(r^{(k)} = \langle \vec{y}^{\,k+1}, \vec{x}^{\,k} \rangle\)</span>
<br>
<span class="math notranslate nohighlight">\(\quad\)</span> <span class="math notranslate nohighlight">\(\vec{x}^{\,k+1} = \vec{y}^{\,k+1}/\| \vec{y}^{\,k+1} \|\)</span>
<br>
end for</p>
<p>(If all goes well) the final values of <span class="math notranslate nohighlight">\(r^{(k)}\)</span> and <span class="math notranslate nohighlight">\(\vec{x}^{\,k}\)</span> approximate <span class="math notranslate nohighlight">\(\lambda_n\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_n\)</span> respectively.</p>
</div>
<div class="section" id="exercise-3">
<h3><span class="section-number">29.3.2. </span>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this headline">¶</a></h3>
<p>Implement this basic algorithm (with a fixed iteration count, as in Example 1),
and then create a second version that imposes an accuracy target (as in Example 2).</p>
</div>
</div>
<div class="section" id="getting-other-eigenvalues-with-the-shifted-inverse-power-method">
<h2><span class="section-number">29.4. </span>Getting other eigenvalues with the Shifted Inverse Power Method<a class="headerlink" href="#getting-other-eigenvalues-with-the-shifted-inverse-power-method" title="Permalink to this headline">¶</a></h2>
<p>The inverse power method computes the eigenvalue closest to 0; by <em>shifting</em>, we can compute the eigenvalue closest to any chosen value <span class="math notranslate nohighlight">\(s\)</span>.
Then by searching various values of <span class="math notranslate nohighlight">\(s\)</span>, we can hope to find all the eigenvectors.
As a variant, once we have <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_n\)</span>, we can search nearby for other large or small eigenvalues: often the few largest and/or the few smallest are most important.</p>
<p>With a symmetric (or Hermitian) matrix, once the eigenvalue of largest magnitude, <span class="math notranslate nohighlight">\(\lambda_1\)</span> is known, the rest are known to be real values in the interval <span class="math notranslate nohighlight">\([-|\lambda_1|,|\lambda_1|]\)</span>, so we know roughly where to seek them.</p>
<p>The main idea here is that for any number <span class="math notranslate nohighlight">\(s\)</span>, matrix <span class="math notranslate nohighlight">\(A - s I\)</span> has eigenvalues
<span class="math notranslate nohighlight">\(\lambda_i  - s\)</span>, with the same eigenvectors as <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[(A - sI)\vec{v}_i = (\lambda_i  - s)\vec{v}_i\]</div>
<p>Thus, applying the inverse power method to <span class="math notranslate nohighlight">\(A - s I\)</span> computes its largest eigenvalue <span class="math notranslate nohighlight">\(\gamma\)</span>,
and then <span class="math notranslate nohighlight">\(\lambda = 1/(\gamma + s)\)</span> is the eigenvalue of <span class="math notranslate nohighlight">\(A\)</span> closest to <span class="math notranslate nohighlight">\(s\)</span>.</p>
<div class="section" id="exercise-4">
<h3><span class="section-number">29.4.1. </span>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this headline">¶</a></h3>
<p>As above, implement this, probably sarting with a fixed iteration count version.</p>
<p>For the test case above, some plausible initial choices for the shifts are each if the entries on the main diagonal,
and as above, testing with <span class="math notranslate nohighlight">\(A_s\)</span></p>
</div>
</div>
<div class="section" id="further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc">
<h2><span class="section-number">29.5. </span>Further topics: getting all the eigenvalues with the QR Method, etc.<a class="headerlink" href="#further-topics-getting-all-the-eigenvalues-with-the-qr-method-etc" title="Permalink to this headline">¶</a></h2>
<p>The above methods are not ideal when many or all of the eigenvalues of a matrix are wanted; then a variety of more advanced methods have been developed, starting with the QR (factorization) Method.</p>
<p>We will not address the details of that method in this course, but one way to think about it for a symmetric matrix is that:</p>
<ul class="simple">
<li><p>The eigenvectors are orthogonal.</p></li>
<li><p>Thus, if after computing <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>, one uses the power iteration starting with <span class="math notranslate nohighlight">\(\vec{x}^{\,0,2}\)</span> orthogonal to <span class="math notranslate nohighlight">\(\vec{v}_1\)</span>, then all the new iterates <span class="math notranslate nohighlight">\(\vec{x}^{\,k,2}\)</span> will stay orthogonal, and one will get the eigenvector corresponding to the largest remaining eigenvector: you get <span class="math notranslate nohighlight">\(\vec{v}_2\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>.</p></li>
<li><p>Continuing likewise, one can get the eigenvalues in descending order of magnitude.</p></li>
<li><p>As a modification, one can do all these almost in parallel: at iteration <span class="math notranslate nohighlight">\(k\)</span>, have an approximation <span class="math notranslate nohighlight">\(\vec{x}^{\,k,i}\)</span> for each <span class="math notranslate nohighlight">\(\lambda_i\)</span> and at each stage, got by adjusting these new approximations so that <span class="math notranslate nohighlight">\(\vec{x}^{\,k,i}\)</span> is orthogonal to all the approximations <span class="math notranslate nohighlight">\(\vec{x}^{\,k,j}\)</span>, <span class="math notranslate nohighlight">\(j &lt; i\)</span>, for all the previous (larger) eigenvalues.
This uses a variant of the <a class="reference external" href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt</a> method for orthogonalizing a set of vectors.</p></li>
</ul>
<hr class="docutils" />
<p>This work is licensed under <a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="minimization-1D.html" title="previous page"><span class="section-number">28. </span>Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction</a>
    <a class='right-next' id="next-link" href="least-squares-fitting-python.html" title="next page"><span class="section-number">30. </span>Least-squares Fitting to Data</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Brenton LeMesurier, College of Charleston and University of Northern Colorado<br/>
        
            &copy; Copyright 2020–2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>