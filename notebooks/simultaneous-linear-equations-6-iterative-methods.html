
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>27. Simultaneous Linear Equations, Part 6: Iterative Methods &#8212; Elementary Numerical Analysis (with Python)</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="28. Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction" href="minimization-1D.html" />
    <link rel="prev" title="26. Simultaneous Linear Equations, Part 5: Error bounds for linear algebra, condition numbers, matrix norms, etc." href="simultaneous-linear-equations-5-error-bounds-condition-numbers-python.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/UNC_BearMascot.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Elementary Numerical Analysis (with Python)</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../preface.html">
   Elementary Numerical Analysis with Python
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../references.html">
   References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../plans-for-expansion-improvement.html">
   Full Disclosure: Things I Plan to do to Expand and Improve This Book
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Numerical Analysis
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-by-interval-halving-python.html">
   1. Root Finding by Interval Halving (Bisection)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fixed-point-iteration-python.html">
   2. Solving Equations by Fixed Point Iteration (of Contraction Mappings)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-python.html">
   3. Newton’s Method for Solving Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="taylors-theorem.html">
   4. Taylor’s Theorem and the Accuracy of Linearization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="error-measures-convergence-rates.html">
   5. Measures of Error and Order of Convergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-convergence-rate.html">
   6. The Convergence Rate of Newton’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-without-derivatives-python.html">
   7. Root-finding Without Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-1-row-reduction-python.html">
   8. Simultaneous Linear Equations, Part 1: Row Reduction/Gaussian Elimination
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machine-numbers-rounding-error-and-error-propagation-python.html">
   9. Machine Numbers, Rounding Error and Error Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-2-pivoting-python.html">
   10. Simultaneous Linear Equations, Part 2: Partial Pivoting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-3-lu-factorization-python.html">
   11. Simultaneous Linear Equations, Part 3: Solving
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
   with LU factorization,
   <span class="math notranslate nohighlight">
    \(A = L U\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-4-plu-factorization-python.html">
   12. Simultaneous Linear Equations, Part 4: Solving
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
   With Both Pivoting and LU factorization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="polynomial-collocation%2Bapproximation-python.html">
   13. Polynomial Collocation (Interpolation/Extrapolation) and Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="polynomial-collocation-error-formulas-python.html">
   14. Error Formulas for Polynomial Collocation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="derivatives-and-the-method-of-undetermined-coefficents.html">
   15. Approximating Derivatives by the Method of Undetermined Coefficients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="richardson-extrapolation.html">
   16. Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-1-building-blocks-python.html">
   17. Definite Integrals, Part 1: The Building Blocks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-2-composite-rules.html">
   18. Definite Integrals, Part 2: The Composite Trapezoid and Midpoint Rules
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-3-simpson-richardson.html">
   19. Definite Integrals, Part 3: The (Composite) Simpson’s Rule and Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="definite-integrals-4-romberg-integration.html">
   20. Definite Integrals, Part 4: Romberg Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-1-basics-and-Euler-python.html">
   21. Initial Value Problems for Ordinary Differential Equations, Part 1: Basic Concepts and Euler’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-2-Runge-Kutta-python.html">
   22. Initial Value Problems for ODEs, Part 2: Runge-Kutta Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-3-error-results-one-step-methods.html">
   23. Initial Value Problems for Ordinary Differential Equations, Part 3: Global Error Bounds for One Step Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-4-system-higher-order-equations.html">
   24. Initial Value Problems for Ordinary Differential Equations, Part 4: Systems of ODEs and Higher Order ODEs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-5-error-control-python.html">
   25. Initial Value Problems for Ordinary Differential Equations, Part 5: Error Control and Variable Step Sizes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-5-error-bounds-condition-numbers-python.html">
   26. Simultaneous Linear Equations, Part 5: Error bounds for linear algebra, condition numbers, matrix norms, etc.
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   27. Simultaneous Linear Equations, Part 6: Iterative Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="minimization-1D.html">
   28. Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eigenproblems.html">
   29. Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../future_topics.html">
   30. Some Future Topics
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Exercises
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-by-interval-halving-exercises-python.html">
   Exercises on the Bisection Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fixed-point-iteration-exercises.html">
   Exercises on Fixed Point Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="error-measures-convergence-rates-exercises.html">
   Exercises on Error Measures and Convergence
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="newtons-method-exercises.html">
   Exercises on Newton’s Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root-finding-without-derivatives-exercises.html">
   Exercises on Root-finding Without Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machine-numbers-rounding-error-and-error-propagation-exercises.html">
   Exercises on Machine Numbers, Rounding Error and Error Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="simultaneous-linear-equations-exercises.html">
   Exercises on Solving Simultaneous Linear Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="derivatives-and-the-method-of-undetermined-coefficents-exercises.html">
   Exercises on Approximating Derivatives, the Method of Undetermined Coefficients and Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ODE-IVP-exercises.html">
   Exercises on Initial Value Problems for Ordinary Differential Equations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="assignment1.html">
   MATH 375 Assignment 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment2.html">
   MATH 375 Assignment 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment3.html">
   MATH 375 Assignment 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="root_finding.html">
   Module root_finding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment4.html">
   MATH 375 Assignment 4
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="assignment5.html">
   MATH 375 Assignment 5
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Project on Numerical Calculus
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-1-and-2-centered-difference-derivative-approximation-and-richardson.html">
   Centered Difference Approximation of the Derivative
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-1-and-2-centered-difference-derivative-approximation-and-richardson.html#improving-on-the-centered-difference-approximation-with-richardson-extrapolation">
   Improving on the Centered Difference Approximation with Richardson Extrapolation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-3-composite-trapezoid-rule.html">
   The Composite Trapezoid Rule (and Composite Midpoint Rule)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-4-recursive-trapezoid-rule.html">
   The Recursive Trapezoid Rule, with error control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-5-and-6-modified-trapezoid-method.html">
   The Modified Trapezoid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/task-7-romberg-method.html">
   The Romberg Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/test-cases-differentiation.html">
   Test Cases for Differentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../project-on-numerical-calculus/test-cases-integration.html">
   Test Cases for Integration
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Some Final Project Possibilities
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html">
   Some Final Project Possibilites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#minimizing-functions-of-one-and-several-variables">
   Minimizing Functions of One and Several Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#root-finding-by-repeated-inverse-quadratic-approximation-with-bracketing">
   Root-finding by Repeated Inverse Quadratic Approximation with Bracketing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#a-more-reliable-secant-method">
   A More Reliable Secant Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#computing-eigenvalues-and-eigenvectors">
   Computing Eigenvalues and Eigenvectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#iterative-methods-for-solving-simultaneous-linear-equations-ax-b">
   Iterative Methods for Solving Simultaneous Linear Equations,
   <span class="math notranslate nohighlight">
    \(Ax = b\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#solving-simultaneous-nonlinear-equations">
   Solving Simultaneous Nonlinear Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#fitting-smooth-piecewise-cubic-functions-to-data">
   Fitting Smooth Piecewise Cubic Functions to Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#least-squares-fitting-to-data-and-functions">
   Least-Squares Fitting to Data and Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="project-possibilities.html#boundary-value-problems-for-differential-equations">
   Boundary Value Problems for Differential Equations
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Python and Jupyter Notebook Review (with Numpy and Matplotlib)
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/introduction.html">
   1. Introduction to
   <em>
    Python Preview
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/getting-python-software-for-scientific-computing.html">
   2. Getting Python Software for Scientific Computing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/suggestions-on-python-and-notebook-usage.html">
   3. Suggestions and Notes on Python and Jupyter Notebook Usage
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/python-variables-lists-tuples-numpy-arrays.html">
   4. Python Variables, Including Lists and Tuples, and Arrays from Package Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/functions.html">
   5. Defining and Using Python Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/decisions-with-if-else-elif.html">
   6. Decision Making With if, else, and elif
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/iteration-with-for.html">
   7. Iteration with
   <code class="docutils literal notranslate">
    <span class="pre">
     for
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/iteration-with-while.html">
   8. Iteration with
   <code class="docutils literal notranslate">
    <span class="pre">
     while
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/code-files-modules-IDEs.html">
   9. Code Files, Modules, and an Integrated Development Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/recursion.html">
   10. Recursion (vs iteration)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/graphing-with-matplotlib.html">
   11. Plotting Graphs with Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/array-operations-and-linear-algebra.html">
   12. Numpy Array Operations and Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/scipy-tools-for-linear-algebra.html">
   13. Package Scipy and More Tools for Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/summation-and-integration.html">
   14. Summation and Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/random-numbers-and-simulation.html">
   15. Random Numbers, Histograms, and a Simulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/formatted-output-and-some-text-string-manipulation.html">
   16. Formatted Output and Some Text String Manipulation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/classes-objects-attributes-methods.html">
   17. Classes, Objects, Attributes, Methods: Very Basic Object-Oriented Programming in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python_tutorial/exception-handling.html">
   18. Exceptions and Exception Handling
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="numerical_methods_module.html">
   Notebook for generating the module
   <code class="docutils literal notranslate">
    <span class="pre">
     numerical_methods_module
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear-algebra-with-0-based-indexing-and-semiopen-intervals.html">
   Linear algebra algorithms using 0-based indexing and semi-open intervals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sample-project-newtons-method-python-draft.html">
   Numerical Analysis Sample Project on Newtons’s Method
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/simultaneous-linear-equations-6-iterative-methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/simultaneous-linear-equations-6-iterative-methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   27.1. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-jacobi-method">
   27.2. The Jacobi Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1-implement-and-test-the-jacobi-method">
     27.2.1. Exercise 1: Implement and test the Jacobi method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-underlying-strategy">
   27.3. The underlying strategy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-does-this-say-about-the-jacobi-method">
     27.3.1. What does this say about the Jacobi method?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gauss-seidel-method">
   27.4. The Gauss-Seidel Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2-implement-and-test-the-gauss-seidel-method-and-compare-to-jacobi">
     27.4.1. Exercise 2: Implement and test the Gauss-Seidel method, and compare to Jacobi
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-family-of-test-cases-arising-from-boundary-value-problems-for-differential-equations">
   27.5. A family of test cases, arising from boundary value problems for differential equations
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="simultaneous-linear-equations-part-6-iterative-methods">
<h1><span class="section-number">27. </span>Simultaneous Linear Equations, Part 6: Iterative Methods<a class="headerlink" href="#simultaneous-linear-equations-part-6-iterative-methods" title="Permalink to this headline">¶</a></h1>
<p><strong>Created on April 13, 2021</strong></p>
<p><strong>Brenton LeMesurier</strong></p>
<p>University of Northern Colorado, Greeley, Colorado in Spring 2021,
<a class="reference external" href="mailto:brenton&#46;lemesurier&#37;&#52;&#48;unco&#46;edu">brenton<span>&#46;</span>lemesurier<span>&#64;</span>unco<span>&#46;</span>edu</a>;
<br>
More permanent address: The College of Charleston, Charleston, South Carolina,
<a class="reference external" href="mailto:lemesurierb&#37;&#52;&#48;cofc&#46;edu">lemesurierb<span>&#64;</span>cofc<span>&#46;</span>edu</a></p>
<div class="section" id="introduction">
<h2><span class="section-number">27.1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This topic is a huge area, with lots of ongoing research; this section just explores the first few methods in the field:</p>
<ol class="simple">
<li><p>The Jacobi Method.</p></li>
<li><p>The Gauss-Seidel Method.</p></li>
</ol>
<p>The next three major topics for further study are:</p>
<ol class="simple">
<li><p>The Method of Succesive Over-Relaxation (“SOR”).
This is usually done as a modification of the Gauss-Seidel method, though the strategy of “over-relaxation” can also be applied to other iterative methods such as the Jacobi method.</p></li>
<li><p>The Conjugate Gradient Method (“CG”).
This is beyond the scope of this course; I mention it because in the realm of solving linear systems that arise in the solution of differential equations, CG and SOR are the basis of many of the most modern, advanced methods.</p></li>
<li><p>Preconditioning.</p></li>
</ol>
</div>
<div class="section" id="the-jacobi-method">
<h2><span class="section-number">27.2. </span>The Jacobi Method<a class="headerlink" href="#the-jacobi-method" title="Permalink to this headline">¶</a></h2>
<p>The basis of the Jacobi method for solving <span class="math notranslate nohighlight">\(Ax = b\)</span>
is splitting <span class="math notranslate nohighlight">\(A\)</span> as <span class="math notranslate nohighlight">\(D + R\)</span> where <span class="math notranslate nohighlight">\(D\)</span> is the diagonal of <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
d_{i,i} &amp;= a_{i,i}
\\
d_{i,j} &amp;= 0, \quad i \neq j
\end{split}\end{split}\]</div>
<p>so that <span class="math notranslate nohighlight">\(R = A-D\)</span> has</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
r_{i,i} &amp;= 0
\\
r_{i,j} &amp;= a_{i, j}, \quad i \neq j
\end{split}\end{split}\]</div>
<p>Visually</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D = \left[ \begin{array}{cccc}
a_{11} &amp; 0 &amp; 0 &amp; \dots
\\
0 &amp; a_{22} &amp; 0 &amp; \dots
\\
0 &amp; 0 &amp; a_{33} &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
\end{split}\]</div>
<p>It is easy to solve <span class="math notranslate nohighlight">\(Dx = b\)</span>: the equations are just <span class="math notranslate nohighlight">\(a_{ii} x_i = b_i\)</span> with solution <span class="math notranslate nohighlight">\(x_i = b_i/a_{ii}\)</span>.</p>
<p>Thus we rewrite the equation
<span class="math notranslate nohighlight">\(Ax = Dx + Rx = b\)</span>
in the fixed point form</p>
<div class="math notranslate nohighlight">
\[Dx = b - Rx\]</div>
<p>and then use the familiar fixed point iteration strategy of inserting the currect approximation at right and solving for the new approximation at left:</p>
<div class="math notranslate nohighlight">
\[D x^{(k)} = b - R x^{(k-1)}\]</div>
<p><strong>Note:</strong> We could make this look closer to the standard fixed-point iteration form <span class="math notranslate nohighlight">\(x_k = g(x_{k-1})\)</span> by dividing out <span class="math notranslate nohighlight">\(D\)</span> to get</p>
<div class="math notranslate nohighlight">
\[x^{(k)} = D^{-1}(b - R x^{(k-1)}),\]</div>
<p>but — as is often the case — it will be better to avoid matrix inverses by instead solving this easy system.
This “inverse avoidance” becomes far more important when we get to the Gauss-Seidel method!</p>
<div class="section" id="exercise-1-implement-and-test-the-jacobi-method">
<h3><span class="section-number">27.2.1. </span>Exercise 1: Implement and test the Jacobi method<a class="headerlink" href="#exercise-1-implement-and-test-the-jacobi-method" title="Permalink to this headline">¶</a></h3>
<p>Write and test Python functions for this.</p>
<p>A) As usual start with a most basic version that does a fixed number of iterations</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x = jacobi_basic(A, b, n)
</pre></div>
</div>
<p>B) Then refine this to apply an error tolerance, but also avoiding infinite loops by imposing an upper limit on the number of iterations:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x = jacobi(A, b, errorTolerance, maxIterations)
</pre></div>
</div>
<p>Test this with the matrices of form <span class="math notranslate nohighlight">\(T\)</span> below for several values of <span class="math notranslate nohighlight">\(n\)</span>, increasingly geometrically.
To be cautious initially, try <span class="math notranslate nohighlight">\(n=2, 4, 8, 16, \dots\)</span></p>
</div>
</div>
<div class="section" id="the-underlying-strategy">
<h2><span class="section-number">27.3. </span>The underlying strategy<a class="headerlink" href="#the-underlying-strategy" title="Permalink to this headline">¶</a></h2>
<p>To analyse the Jacobi method — answetin queogt like for which matices it works, and how quickly — and also to improve on it, it helps to described a key strategy underlying it, which is this:
approximate the matrix <span class="math notranslate nohighlight">\(A\)</span> by another one <span class="math notranslate nohighlight">\(E\)</span> one that is easier to solve with, chosne so that the discrepacy <span class="math notranslate nohighlight">\(R = A-E\)</span> is small enough. Thus repeatedly solving the new easier equations <span class="math notranslate nohighlight">\(Ex^{(k)} = b^{(k)}\)</span> play a similar role to repeatedly solving  the tangent line approximation in Newton’s method.</p>
<p>Of course to be of any use, <span class="math notranslate nohighlight">\(E\)</span> must be somewhat close to <span class="math notranslate nohighlight">\(A\)</span>; the remainder <span class="math notranslate nohighlight">\(R\)</span> must be small enough.
We can make this requirment precise with the use of
<a class="reference external" href="simultaneous-linear-equations-5-error-bounds-condition-numbers-python.ipynb#matrix-norms">matrix norms</a>
and an upgrade of the contraction mapping theorem in the section on
<a class="reference internal" href="fixed-point-iteration-python.html"><span class="doc std std-doc">fixed-point iteration</span></a>.</p>
<p>Thus consider a general <em>splitting</em> of <span class="math notranslate nohighlight">\(A\)</span> as <span class="math notranslate nohighlight">\(A = E + R\)</span>.
As abve we rewrite <span class="math notranslate nohighlight">\(Ax = Ex + Rx = b\)</span> as <span class="math notranslate nohighlight">\(Ex = b - Rx\)</span> and thence as <span class="math notranslate nohighlight">\(x = E^{-1}b - (E^{-1}R)x\)</span>.
(It is alright to use the matrix inverse here, since we are not actually computing it; only using it for a theroretical argument!)
The fixed point iteration form is thus</p>
<div class="math notranslate nohighlight">
\[x^{(k)} = g(x^{(k-1)}) = c - S x^{(k-1)}\]</div>
<p>where <span class="math notranslate nohighlight">\(c = E^{-1}b\)</span> and <span class="math notranslate nohighlight">\(S = E^{-1}R\)</span>.</p>
<p>For vector-valued functions we extend the previous definition as:</p>
<p><strong>Definition 1: Vector-valued Contraction Mapping</strong>.
For a set <span class="math notranslate nohighlight">\(D\)</span> of <span class="math notranslate nohighlight">\(n\)</span>-vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>,
a mapping <span class="math notranslate nohighlight">\(g:D \to D\)</span> is called a <em>contraction</em> or <em>contraction mapping</em> if there is a constant <span class="math notranslate nohighlight">\(C &lt; 1\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\|g(x) - g(y)\| \leq C \|x - y\|\]</div>
<p>for any <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> in <span class="math notranslate nohighlight">\(D\)</span>.
We then call <span class="math notranslate nohighlight">\(C\)</span> a <em>contraction constant</em>.</p>
<p>and extend the Contraction Mapping Theorem to</p>
<p><strong>Theorem 1. (Contraction Mapping Theorem for vector-valued functions).</strong>
Any contraction mapping <span class="math notranslate nohighlight">\(g\)</span> on a closed, bounded set <span class="math notranslate nohighlight">\(D \in \mathbb{R}^n\)</span> has exactly one fixed point <span class="math notranslate nohighlight">\(p\)</span> in <span class="math notranslate nohighlight">\(D\)</span>.
Further, this can be calculated as the limit <span class="math notranslate nohighlight">\(\displaystyle p = \lim_{k \to \infty} x^{(k)}\)</span> of the iteration sequence given by <span class="math notranslate nohighlight">\(x^{(k+1)} = g(x^{(k)})\)</span> for <em>any</em> choice of the starting point <span class="math notranslate nohighlight">\(x^{(0)} \in D\)</span>.</p>
<p>Further the errors decrease at a guaranteed minimum speed:
<span class="math notranslate nohighlight">\(\| x^{(k+1)} - p \| \leq C \| x^{(k)} - p \|\)</span>, so <span class="math notranslate nohighlight">\(\| x^{(k)} - p \| \leq C^k  \| x^{(0)} - p \|\)</span>.</p>
<p>With this, it turns out that the above iteration converges if <span class="math notranslate nohighlight">\(S\)</span> is “small enough” in the sense that <span class="math notranslate nohighlight">\(\|S\| = C &lt; 1\)</span> — and it is enough that this works for <em>any</em> choice of matrix norm!</p>
<p><strong>Theorem 2.</strong>
If <span class="math notranslate nohighlight">\(S := E^{-1}R = E^{-1}A - I\)</span> has <span class="math notranslate nohighlight">\(\|S\| = C &lt; 1\)</span> for any choice of matrix norm,
then the iterative scheme <span class="math notranslate nohighlight">\(x^{(k)} = c - S x^{(k-1)}\)</span> with <span class="math notranslate nohighlight">\(c = E^{-1}b\)</span> converges to the solution of
<span class="math notranslate nohighlight">\(Ax = b\)</span> for any choice of the initial approximation <span class="math notranslate nohighlight">\(x^{(0)}\)</span>.
(Aside: the zero vector is an obvious and popular choice for <span class="math notranslate nohighlight">\(x^{(0)}\)</span>.)</p>
<p>Incidentally this condition also guarantees that <span class="math notranslate nohighlight">\(A\)</span> is non-singular so that there exists a unique solution.</p>
<p><strong>Main Proof Idea:</strong>
For <span class="math notranslate nohighlight">\(g(x) = c - S x\)</span>,</p>
<div class="math notranslate nohighlight">
\[\|g(x) - g(y)\| = \| (c - S x) - (c - S y) \| = \| S(y - x) \| \leq \|S\| \|y-x\| \leq C \|x - y\|,\]</div>
<p>so with <span class="math notranslate nohighlight">\(C &lt; 1\)</span>, it is a contraction.</p>
<p>(The omitted more “technical” detail is to find a suitable bounded domain <span class="math notranslate nohighlight">\(D\)</span> that all the iterates x^{(k)} stay inside.)</p>
<div class="section" id="what-does-this-say-about-the-jacobi-method">
<h3><span class="section-number">27.3.1. </span>What does this say about the Jacobi method?<a class="headerlink" href="#what-does-this-say-about-the-jacobi-method" title="Permalink to this headline">¶</a></h3>
<p>For the Jacobi method, <span class="math notranslate nohighlight">\(E = D\)</span> so <span class="math notranslate nohighlight">\(E^{-1}\)</span> is the diagonal matrix with elements <span class="math notranslate nohighlight">\(1/a_{i,i}\)</span> on the main diagonal, zero elsewhere.
The product <span class="math notranslate nohighlight">\(E^{-1}A\)</span> then multiplies each row <span class="math notranslate nohighlight">\(i\)</span> by <span class="math notranslate nohighlight">\(1/a_{i,i}\)</span>, giving</p>
<div class="math notranslate nohighlight">
\[\begin{split}
E^{-1}A = \left[ \begin{array}{cccc}
1 &amp; a_{1,2}/a_{1,1} &amp; a_{1,2}/a_{1,1} &amp; \dots
\\
a_{2,1}/a_{2,2} &amp; 1 &amp; a_{2,3}/a_{2,2} &amp; \dots
\\
a_{3,1}/a_{3,3} &amp; a_{3,2}/a_{3,3} &amp; 1 &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
\end{split}\]</div>
<p>so that subtracting the identity matrix to get <span class="math notranslate nohighlight">\(S\)</span> cancels the ones on the main diagonal:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
S = E^{-1}A - I= \left[ \begin{array}{cccc}
0 &amp; a_{1,2}/a_{1,1} &amp; a_{1,2}/a_{1,1} &amp; \dots
\\
a_{2,1}/a_{2,2} &amp; 0 &amp; a_{2,3}/a_{2,2} &amp; \dots
\\
a_{3,1}/a_{3,3} &amp; a_{3,2}/a_{3,3} &amp; 0 &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
\end{split}\]</div>
<p>Here is one of many places that using the maximum-norm, a.k.a. <span class="math notranslate nohighlight">\(\infty\)</span>-norm, makes life much easier!</p>
<ul class="simple">
<li><p>First, sum the absolute values of elements in each row <span class="math notranslate nohighlight">\(i\)</span>; with the common factor <span class="math notranslate nohighlight">\(1/|a_{i,i}|\)</span>,
this gives
<span class="math notranslate nohighlight">\( (|a_{i,1}| + |a_{i,2}| + \cdots |a_{i,i-1}| + |a_{i,i+1}| + \cdots |a_{i,n}|) \)</span>.
Such a sum, skipping index <span class="math notranslate nohighlight">\(j=i\)</span>, can be abbreviated as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\left( \sum_{1 \leq j \leq n, j \neq i}|a_{i,1}| \right) / |a_{i,i}|\]</div>
<ul class="simple">
<li><p>Then get the <span class="math notranslate nohighlight">\(\infty\)</span>-norm as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[C = \max_{i=1}^n \left( \sum_{1 \leq j \leq n, j \neq i}|a_{i,j}| \right) / |a_{i,i}|\]</div>
<p>and the contraction condition <span class="math notranslate nohighlight">\(C &lt; 1\)</span> becomes the requirement that each of these <span class="math notranslate nohighlight">\(n\)</span> “row sums” is less than 1:</p>
<p>Multiplying each of the inequalities by the denominator <span class="math notranslate nohighlight">\(|a_{i,i}|\)</span> gives <span class="math notranslate nohighlight">\(n\)</span> conditions</p>
<div class="math notranslate nohighlight">
\[\left( \sum_{1 \leq j \leq n, j \neq i}|a_{i,j}| \right) &lt; |a_{i,i}|\]</div>
<p>In words, the magnitude of each main diagonal element is greater than the <strong>sum</strong> of the magnitudes of all other elements in that row.
Intuitively, the elements of the main diagonal <em>dominate</em> the rest of their row in size;
hence the name for such a matrix:</p>
<p><strong>Definition 2: (Row-wise) Strictly Diagonally Dominant</strong>.
A matrix <span class="math notranslate nohighlight">\(A\)</span> is <em>Row-wise Strictly Diagonally Dominant</em> (sometimes abbreviated as just <em>Strictly Diagonally Dominant</em> or even just <em>SDD</em>) if</p>
<div class="math notranslate nohighlight">
\[\sum_{1 \leq j \leq n, j \neq i}|a_{i,j}| &lt; |a_{i,i}|\]</div>
<p>Another way to think of this is that such a matrix <span class="math notranslate nohighlight">\(A\)</span> is close to its main diagonal <span class="math notranslate nohighlight">\(D\)</span>,
which is the intuitive condition that the approximation of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(D\)</span> as done in the Jacobi method is “good enough”.
And indeed, combining this result with Theorem 2 gives:</p>
<p><strong>Theorem 3.</strong>
The Jacobi Method converges if <span class="math notranslate nohighlight">\(A\)</span> is Strictly Diagonally Dominant, for any initial approximation <span class="math notranslate nohighlight">\(x^{(0)}\)</span>.</p>
<p>Further, the error goes down by at least a factor of <span class="math notranslate nohighlight">\(\| I - D^{-1}A \|\)</span> at each iteration.</p>
<p>By the way, other matrix norms give other conditions guaranteeing convergence; perhaps the most useful of these others is that it is also sufficient for <span class="math notranslate nohighlight">\(A\)</span> to be <em>Column-wise Strictly Diagonally Dominant</em> — I leave it to you to guess the definition of that!</p>
</div>
</div>
<div class="section" id="the-gauss-seidel-method">
<h2><span class="section-number">27.4. </span>The Gauss-Seidel Method<a class="headerlink" href="#the-gauss-seidel-method" title="Permalink to this headline">¶</a></h2>
<p>To recap, two key ingredients for a splitting <span class="math notranslate nohighlight">\(A = E + R\)</span> to be useful are that</p>
<ul class="simple">
<li><p>the matrix <span class="math notranslate nohighlight">\(E\)</span> is “easy” to solve with, and</p></li>
<li><p>it is not too far from <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<p>The Jacobi method choice of <span class="math notranslate nohighlight">\(E\)</span> being the main diagonal of <span class="math notranslate nohighlight">\(A\)</span> strongly emphasizes the “easy” part, but we have seen another larger class of matrices for which it is fairly quick and easy to solve <span class="math notranslate nohighlight">\(Ex = b\)</span>: <em>triangular matrices</em>, which can be solved with forward or backward substitution, not needing row reduction.</p>
<p>The Gauss-Seidel Method takes <span class="math notranslate nohighlight">\(E\)</span> be the lower triangular part of <span class="math notranslate nohighlight">\(A\)</span>, which intuitively leaves more of its entries closer to <span class="math notranslate nohighlight">\(A\)</span> and makes the remainder <span class="math notranslate nohighlight">\(R = A-E\)</span> “smaller”.</p>
<p>To discuss this and other splittings, we write the matrix as <span class="math notranslate nohighlight">\(A = L + D + U\)</span> where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the diagonal of <span class="math notranslate nohighlight">\(A\)</span>, as for Jacobi</p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span> is the strictly lower diagonal part of <span class="math notranslate nohighlight">\(A\)</span> (just the elements with <span class="math notranslate nohighlight">\(i &gt; j\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(U\)</span> is the strictly upper diagonal part of <span class="math notranslate nohighlight">\(A\)</span> (just the elements with <span class="math notranslate nohighlight">\(i &lt; j\)</span>)</p></li>
</ul>
<p>That is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A =
\left[ \begin{array}{cccc}
a_{1,1} &amp; a_{1,2} &amp; a_{1,3} &amp; \dots
\\
a_{2,1} &amp; a_{2,2} &amp; a_{2,3} &amp; \dots
\\
a_{3,1} &amp; a_{3,2} &amp; a_{3,3} &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
= \left[ \begin{array}{cccc}
0 &amp; 0 &amp; 0 &amp; \dots
\\
a_{2,1} &amp; 0 &amp; 0 &amp; \dots
\\
a_{3,1} &amp; a_{3,2} &amp; 0 &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
+ \left[ \begin{array}{cccc}
a_{1,1} &amp; 0 &amp; 0 &amp; \dots
\\
0 &amp; a_{2,2} &amp; 0 &amp; \dots
\\
0 &amp; 0 &amp; a_{3,3} &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
+
\left[ \begin{array}{cccc}
0 &amp; a_{1,2} &amp; a_{1,3} &amp; \dots
\\
0 &amp; 0 &amp; a_{2,3} &amp; \dots
\\
0 &amp; 0 &amp; 0 &amp; \dots
\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{array} \right]
= L + D + U
\end{split}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(R = L+U\)</span> for the Jacobi method.</p>
<p>So now we use <span class="math notranslate nohighlight">\(E = L + D\)</span>, which will be called <span class="math notranslate nohighlight">\(A_L\)</span>, the <em>lower triangular part of <span class="math notranslate nohighlight">\(A\)</span></em>, and the remainder is <span class="math notranslate nohighlight">\(R = U\)</span>.
The fixed point form becomes</p>
<div class="math notranslate nohighlight">
\[A_L x = b - Ux\]</div>
<p>giving the fixed point iteration</p>
<div class="math notranslate nohighlight">
\[A_L x^{(k)} = b - U x^{(k-1)}\]</div>
<p>Here we definitely do not use the inverse of <span class="math notranslate nohighlight">\(A_L\)</span> when calculating!
Instead, solve with forward substitution.</p>
<p>However to analyse convergence, the mathematical form</p>
<div class="math notranslate nohighlight">
\[x^{(k)} = A_L^{-1}b - (A_L^{-1}U) x^{(k-1)}\]</div>
<p>is useful: the iteration map is now <span class="math notranslate nohighlight">\(g(x) = c - S x\)</span> with <span class="math notranslate nohighlight">\(c = (L + D)^{-1} b\)</span> and <span class="math notranslate nohighlight">\(S = (L + D)^{-1} U\)</span>.</p>
<p>Arguing as above, we see that convergence is guaranteed if <span class="math notranslate nohighlight">\(\| (L + D)^{-1} U\| &lt; 1\)</span>.
However it is not so easy in general to get a formulas for <span class="math notranslate nohighlight">\(\| (L + D)^{-1} U\|\)</span>;
what one can get is slightly disappointing in that, despite the <span class="math notranslate nohighlight">\(R=U\)</span> here being in some sense “smaller” than the <span class="math notranslate nohighlight">\(R=L+U\)</span> for the Jacobi method, the general convergence guarantee looks no better:</p>
<p><strong>Theorem 3.</strong>
The Gauss-Seidel method converges if <span class="math notranslate nohighlight">\(A\)</span> is Strictly Diagonally Dominant, for any initial approximation <span class="math notranslate nohighlight">\(x^{(0)}\)</span>.</p>
<p>However, in practice the convergence rate as given by <span class="math notranslate nohighlight">\(C = C_{GS} = \| (L + D)^{-1} U\|\)</span> is often better than for the
<span class="math notranslate nohighlight">\(C = C_J = \| D^{-1} (L+U) \|\)</span> for the Jacobi method.</p>
<p>Sometimes this reduce the number of iterations enough to outweigh the extra computational effort involved in each iteration and make this faster overall than the Jacobi method — but not always.</p>
<div class="section" id="exercise-2-implement-and-test-the-gauss-seidel-method-and-compare-to-jacobi">
<h3><span class="section-number">27.4.1. </span>Exercise 2: Implement and test the Gauss-Seidel method, and compare to Jacobi<a class="headerlink" href="#exercise-2-implement-and-test-the-gauss-seidel-method-and-compare-to-jacobi" title="Permalink to this headline">¶</a></h3>
<p>Do the two versions as above and use the same test cases.</p>
<p>Then compare the speed/cost of the two methods:
one way to do this is by using Python’s “stop watch”, function <code class="docutils literal notranslate"><span class="pre">time.time</span></code>: see the description of
<a class="reference external" href="https://docs.python.org/3/library/time.html">Python module time</a> in the Python manual.</p>
</div>
</div>
<div class="section" id="a-family-of-test-cases-arising-from-boundary-value-problems-for-differential-equations">
<h2><span class="section-number">27.5. </span>A family of test cases, arising from boundary value problems for differential equations<a class="headerlink" href="#a-family-of-test-cases-arising-from-boundary-value-problems-for-differential-equations" title="Permalink to this headline">¶</a></h2>
<p>The <em>tri-diagonal</em> matrices <span class="math notranslate nohighlight">\(T\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
t_{i,i} &amp;= 1 + 2 h^2
\\
t_{i,i+1} = t_{i,i+1} &amp;= -h^2
\\
t_{i,j} &amp;= 0, \quad |i-j|&gt; 1
\end{split}\end{split}\]</div>
<p>and variants of this arise in the solutions of boundary value problems for ODEs like</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
-u''(x) + K u &amp;= f(x), \quad a \leq x \leq b
\\
u(a) = u(b) &amp;= 0
\end{split}\end{split}\]</div>
<p>and related problems for partial differential equations.</p>
<p>Thus these provide useful initial test cases — usually with <span class="math notranslate nohighlight">\(h = (b-a)/n\)</span>.</p>
<hr class="docutils" />
<p>This work is licensed under <a class="reference external" href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International</a></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="simultaneous-linear-equations-5-error-bounds-condition-numbers-python.html" title="previous page"><span class="section-number">26. </span>Simultaneous Linear Equations, Part 5: Error bounds for linear algebra, condition numbers, matrix norms, etc.</a>
    <a class='right-next' id="next-link" href="minimization-1D.html" title="next page"><span class="section-number">28. </span>Finding the Minimum of a Function of One Variable Without Using Derivatives — A Brief Introduction</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Brenton LeMesurier, College of Charleston and University of Northern Colorado<br/>
        
            &copy; Copyright 2020–2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>