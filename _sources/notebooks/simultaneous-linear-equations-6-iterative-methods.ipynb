{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "offensive-tribune",
   "metadata": {},
   "source": [
    "# Simultaneous Linear Equations, Part 6: Iterative Methods\n",
    "\n",
    "**Created on April 13, 2021**\n",
    "\n",
    "**Brenton LeMesurier**\n",
    "\n",
    "University of Northern Colorado, Greeley, Colorado in Spring 2021,\n",
    "[brenton.lemesurier@unco.edu](mailto:brenton.lemesurier@unco.edu);\n",
    "<br>\n",
    "More permanent address: The College of Charleston, Charleston, South Carolina,\n",
    "[lemesurierb@cofc.edu](mailto:lemesurierb@cofc.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-scale",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This topic is a huge area, with lots of ongoing research; this section just explores the first few methods in the field:\n",
    "\n",
    "1. The Jacobi Method.\n",
    "\n",
    "2. The Gauss-Seidel Method.\n",
    "\n",
    "The next three major topics for further study are:\n",
    "\n",
    "3. The Method of Succesive Over-Relaxation (\"SOR\").\n",
    "This is usually done as a modification of the Gauss-Seidel method, though the strategy of \"over-relaxation\" can also be applied to other iterative methods such as the Jacobi method.\n",
    "\n",
    "4. The Conjugate Gradient Method (\"CG\").\n",
    "This is beyond the scope of this course; I mention it because in the realm of solving linear systems that arise in the solution of differential equations, CG and SOR are the basis of many of the most modern, advanced methods.\n",
    "\n",
    "5. Preconditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-median",
   "metadata": {},
   "source": [
    "## The Jacobi Method\n",
    "\n",
    "The basis of the Jacobi method for solving $Ax = b$\n",
    "is splitting $A$ as $D + R$ where $D$ is the diagonal of $A$:\n",
    "\n",
    "$$\\begin{split}\n",
    "d_{i,i} &= a_{i,i}\n",
    "\\\\\n",
    "d_{i,j} &= 0, \\quad i \\neq j\n",
    "\\end{split}$$\n",
    "\n",
    "so that $R = A-D$ has\n",
    "\n",
    "$$\\begin{split}\n",
    "r_{i,i} &= 0\n",
    "\\\\\n",
    "r_{i,j} &= a_{i, j}, \\quad i \\neq j\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-patrol",
   "metadata": {},
   "source": [
    "Visually\n",
    "\n",
    "$$\n",
    "D = \\left[ \\begin{array}{cccc}\n",
    "a_{11} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "0 & a_{22} & 0 & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{33} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-applicant",
   "metadata": {},
   "source": [
    "It is easy to solve $Dx = b$: the equations are just $a_{ii} x_i = b_i$ with solution $x_i = b_i/a_{ii}$.\n",
    "\n",
    "Thus we rewrite the equation\n",
    "$Ax = Dx + Rx = b$\n",
    "in the fixed point form\n",
    "\n",
    "$$Dx = b - Rx$$\n",
    "\n",
    "and then use the familiar fixed point iteration strategy of inserting the currect approximation at right and solving for the new approximation at left:\n",
    "\n",
    "$$D x^{(k)} = b - R x^{(k-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-drive",
   "metadata": {},
   "source": [
    "**Note:** We could make this look closer to the standard fixed-point iteration form $x_k = g(x_{k-1})$ by dividing out $D$ to get\n",
    "\n",
    "$$x^{(k)} = D^{-1}(b - R x^{(k-1)}),$$\n",
    "\n",
    "but — as is often the case — it will be better to avoid matrix inverses by instead solving this easy system.\n",
    "This \"inverse avoidance\" becomes far more important when we get to the Gauss-Seidel method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-madness",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement and test the Jacobi method\n",
    "\n",
    "Write and test Python functions for this.\n",
    "\n",
    "A) As usual start with a most basic version that does a fixed number of iterations\n",
    "\n",
    "    x = jacobi_basic(A, b, n)\n",
    "    \n",
    "B) Then refine this to apply an error tolerance, but also avoiding infinite loops by imposing an upper limit on the number of iterations:\n",
    "\n",
    "    x = jacobi(A, b, errorTolerance, maxIterations)\n",
    "    \n",
    "Test this with the matrices of form $T$ below for several values of $n$, increasingly geometrically.\n",
    "To be cautious initially, try $n=2, 4, 8, 16, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-direction",
   "metadata": {},
   "source": [
    "## The underlying strategy\n",
    "\n",
    "To analyse the Jacobi method — answetin queogt like for which matices it works, and how quickly — and also to improve on it, it helps to described a key strategy underlying it, which is this:\n",
    "approximate the matrix $A$ by another one $E$ one that is easier to solve with, chosne so that the discrepacy $R = A-E$ is small enough. Thus repeatedly solving the new easier equations $Ex^{(k)} = b^{(k)}$ play a similar role to repeatedly solving  the tangent line approximation in Newton's method.\n",
    "\n",
    "Of course to be of any use, $E$ must be somewhat close to $A$; the remainder $R$ must be small enough.\n",
    "We can make this requirment precise with the use of\n",
    "[matrix norms](simultaneous-linear-equations-5-error-bounds-condition-numbers-python.ipynb#matrix-norms)\n",
    "and an upgrade of the contraction mapping theorem in the section on\n",
    "[fixed-point iteration](fixed-point-iteration-python.ipynb).\n",
    "\n",
    "Thus consider a general *splitting* of $A$ as $A = E + R$.\n",
    "As abve we rewrite $Ax = Ex + Rx = b$ as $Ex = b - Rx$ and thence as $x = E^{-1}b - (E^{-1}R)x$.\n",
    "(It is alright to use the matrix inverse here, since we are not actually computing it; only using it for a theroretical argument!)\n",
    "The fixed point iteration form is thus\n",
    "\n",
    "$$x^{(k)} = g(x^{(k-1)}) = c - S x^{(k-1)}$$\n",
    "\n",
    "where $c = E^{-1}b$ and $S = E^{-1}R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-apparatus",
   "metadata": {},
   "source": [
    "For vector-valued functions we extend the previous definition as:\n",
    "\n",
    "**Definition 1: Vector-valued Contraction Mapping**.\n",
    "For a set $D$ of $n$-vectors in $\\mathbb{R}^n$,\n",
    "a mapping $g:D \\to D$ is called a *contraction* or *contraction mapping* if there is a constant $C < 1$ such that\n",
    "\n",
    "$$\\|g(x) - g(y)\\| \\leq C \\|x - y\\|$$\n",
    "\n",
    "for any $x$ and $y$ in $D$.\n",
    "We then call $C$ a *contraction constant*.\n",
    "\n",
    "and extend the Contraction Mapping Theorem to\n",
    "\n",
    "**Theorem 1. (Contraction Mapping Theorem for vector-valued functions).**\n",
    "Any contraction mapping $g$ on a closed, bounded set $D \\in \\mathbb{R}^n$ has exactly one fixed point $p$ in $D$.\n",
    "Further, this can be calculated as the limit $\\displaystyle p = \\lim_{k \\to \\infty} x^{(k)}$ of the iteration sequence given by $x^{(k+1)} = g(x^{(k)})$ for *any* choice of the starting point $x^{(0)} \\in D$.\n",
    "\n",
    "Further the errors decrease at a guaranteed minimum speed:\n",
    "$\\| x^{(k+1)} - p \\| \\leq C \\| x^{(k)} - p \\|$, so $\\| x^{(k)} - p \\| \\leq C^k  \\| x^{(0)} - p \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-turtle",
   "metadata": {},
   "source": [
    "With this, it turns out that the above iteration converges if $S$ is \"small enough\" in the sense that $\\|S\\| = C < 1$ — and it is enough that this works for *any* choice of matrix norm!\n",
    "\n",
    "**Theorem 2.**\n",
    "If $S := E^{-1}R = E^{-1}A - I$ has $\\|S\\| = C < 1$ for any choice of matrix norm,\n",
    "then the iterative scheme $x^{(k)} = c - S x^{(k-1)}$ with $c = E^{-1}b$ converges to the solution of\n",
    "$Ax = b$ for any choice of the initial approximation $x^{(0)}$.\n",
    "(Aside: the zero vector is an obvious and popular choice for $x^{(0)}$.)\n",
    "\n",
    "Incidentally this condition also guarantees that $A$ is non-singular so that there exists a unique solution.\n",
    "\n",
    "**Main Proof Idea:**\n",
    "For $g(x) = c - S x$,\n",
    "\n",
    "$$\\|g(x) - g(y)\\| = \\| (c - S x) - (c - S y) \\| = \\| S(y - x) \\| \\leq \\|S\\| \\|y-x\\| \\leq C \\|x - y\\|,$$\n",
    "\n",
    "so with $C < 1$, it is a contraction.\n",
    "\n",
    "(The omitted more \"technical\" detail is to find a suitable bounded domain $D$ that all the iterates x^{(k)} stay inside.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-leone",
   "metadata": {},
   "source": [
    "### What does this say about the Jacobi method?\n",
    "\n",
    "For the Jacobi method, $E = D$ so $E^{-1}$ is the diagonal matrix with elements $1/a_{i,i}$ on the main diagonal, zero elsewhere.\n",
    "The product $E^{-1}A$ then multiplies each row $i$ by $1/a_{i,i}$, giving\n",
    "\n",
    "$$\n",
    "E^{-1}A = \\left[ \\begin{array}{cccc}\n",
    "1 & a_{1,2}/a_{1,1} & a_{1,2}/a_{1,1} & \\dots\n",
    "\\\\\n",
    "a_{2,1}/a_{2,2} & 1 & a_{2,3}/a_{2,2} & \\dots\n",
    "\\\\\n",
    "a_{3,1}/a_{3,3} & a_{3,2}/a_{3,3} & 1 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "so that subtracting the identity matrix to get $S$ cancels the ones on the main diagonal:\n",
    "\n",
    "$$\n",
    "S = E^{-1}A - I= \\left[ \\begin{array}{cccc}\n",
    "0 & a_{1,2}/a_{1,1} & a_{1,2}/a_{1,1} & \\dots\n",
    "\\\\\n",
    "a_{2,1}/a_{2,2} & 0 & a_{2,3}/a_{2,2} & \\dots\n",
    "\\\\\n",
    "a_{3,1}/a_{3,3} & a_{3,2}/a_{3,3} & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-emission",
   "metadata": {},
   "source": [
    "Here is one of many places that using the maximum-norm, a.k.a. $\\infty$-norm, makes life much easier!\n",
    "\n",
    "- First, sum the absolute values of elements in each row $i$; with the common factor $1/|a_{i,i}|$,\n",
    "this gives\n",
    "$ (|a_{i,1}| + |a_{i,2}| + \\cdots |a_{i,i-1}| + |a_{i,i+1}| + \\cdots |a_{i,n}|) $.\n",
    "Such a sum, skipping index $j=i$, can be abbreviated as\n",
    "\n",
    "$$\\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,1}| \\right) / |a_{i,i}|$$\n",
    "\n",
    "- Then get the $\\infty$-norm as\n",
    "\n",
    "$$C = \\max_{i=1}^n \\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| \\right) / |a_{i,i}|$$\n",
    "\n",
    "and the contraction condition $C < 1$ becomes the requirement that each of these $n$ \"row sums\" is less than 1:\n",
    "\n",
    "Multiplying each of the inequalities by the denominator $|a_{i,i}|$ gives $n$ conditions\n",
    "\n",
    "$$\\left( \\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| \\right) < |a_{i,i}|$$\n",
    "\n",
    "In words, the magnitude of each main diagonal element is greater than the **sum** of the magnitudes of all other elements in that row.\n",
    "Intuitively, the elements of the main diagonal *dominate* the rest of their row in size;\n",
    "hence the name for such a matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-neighbor",
   "metadata": {},
   "source": [
    "**Definition 2: (Row-wise) Strictly Diagonally Dominant**.\n",
    "A matrix $A$ is *Row-wise Strictly Diagonally Dominant* (sometimes abbreviated as just *Strictly Diagonally Dominant* or even just *SDD*) if \n",
    "\n",
    "$$\\sum_{1 \\leq j \\leq n, j \\neq i}|a_{i,j}| < |a_{i,i}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-religion",
   "metadata": {},
   "source": [
    "Another way to think of this is that such a matrix $A$ is close to its main diagonal $D$,\n",
    "which is the intuitive condition that the approximation of $A$ by $D$ as done in the Jacobi method is \"good enough\".\n",
    "And indeed, combining this result with Theorem 2 gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-worst",
   "metadata": {},
   "source": [
    "**Theorem 3.**\n",
    "The Jacobi Method converges if $A$ is Strictly Diagonally Dominant, for any initial approximation $x^{(0)}$.\n",
    "\n",
    "Further, the error goes down by at least a factor of $\\| I - D^{-1}A \\|$ at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-scope",
   "metadata": {},
   "source": [
    "By the way, other matrix norms give other conditions guaranteeing convergence; perhaps the most useful of these others is that it is also sufficient for $A$ to be *Column-wise Strictly Diagonally Dominant* — I leave it to you to guess the definition of that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-footage",
   "metadata": {},
   "source": [
    "## The Gauss-Seidel Method\n",
    "\n",
    "To recap, two key ingredients for a splitting $A = E + R$ to be useful are that\n",
    "- the matrix $E$ is \"easy\" to solve with, and\n",
    "- it is not too far from $A$.\n",
    "\n",
    "The Jacobi method choice of $E$ being the main diagonal of $A$ strongly emphasizes the \"easy\" part, but we have seen another larger class of matrices for which it is fairly quick and easy to solve $Ex = b$: *triangular matrices*, which can be solved with forward or backward substitution, not needing row reduction.\n",
    "\n",
    "The Gauss-Seidel Method takes $E$ be the lower triangular part of $A$, which intuitively leaves more of its entries closer to $A$ and makes the remainder $R = A-E$ \"smaller\".\n",
    "\n",
    "To discuss this and other splittings, we write the matrix as $A = L + D + U$ where:\n",
    "- $D$ is the diagonal of $A$, as for Jacobi\n",
    "- $L$ is the strictly lower diagonal part of $A$ (just the elements with $i > j$)\n",
    "- $U$ is the strictly upper diagonal part of $A$ (just the elements with $i < j$)\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\left[ \\begin{array}{cccc}\n",
    "a_{1,1} & a_{1,2} & a_{1,3} & \\dots\n",
    "\\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} & \\dots\n",
    "\\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{cccc}\n",
    "0 & 0 & 0 & \\dots\n",
    "\\\\\n",
    "a_{2,1} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "a_{3,1} & a_{3,2} & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "+ \\left[ \\begin{array}{cccc}\n",
    "a_{1,1} & 0 & 0 & \\dots\n",
    "\\\\\n",
    "0 & a_{2,2} & 0 & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{3,3} & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "+\n",
    "\\left[ \\begin{array}{cccc}\n",
    "0 & a_{1,2} & a_{1,3} & \\dots\n",
    "\\\\\n",
    "0 & 0 & a_{2,3} & \\dots\n",
    "\\\\\n",
    "0 & 0 & 0 & \\dots\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{array} \\right]\n",
    "= L + D + U\n",
    "$$\n",
    "\n",
    "Thus $R = L+U$ for the Jacobi method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-regulation",
   "metadata": {},
   "source": [
    "So now we use $E = L + D$, which will be called $A_L$, the *lower triangular part of $A$*, and the remainder is $R = U$.\n",
    "The fixed point form becomes\n",
    "\n",
    "$$A_L x = b - Ux$$\n",
    "\n",
    "giving the fixed point iteration\n",
    "\n",
    "$$A_L x^{(k)} = b - U x^{(k-1)}$$\n",
    "\n",
    "Here we definitely do not use the inverse of $A_L$ when calculating!\n",
    "Instead, solve with forward substitution.\n",
    "\n",
    "However to analyse convergence, the mathematical form\n",
    "\n",
    "$$x^{(k)} = A_L^{-1}b - (A_L^{-1}U) x^{(k-1)}$$\n",
    "\n",
    "is useful: the iteration map is now $g(x) = c - S x$ with $c = (L + D)^{-1} b$ and $S = (L + D)^{-1} U$.\n",
    "\n",
    "Arguing as above, we see that convergence is guaranteed if $\\| (L + D)^{-1} U\\| < 1$.\n",
    "However it is not so easy in general to get a formulas for $\\| (L + D)^{-1} U\\|$;\n",
    "what one can get is slightly disappointing in that, despite the $R=U$ here being in some sense \"smaller\" than the $R=L+U$ for the Jacobi method, the general convergence guarantee looks no better:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-village",
   "metadata": {},
   "source": [
    "**Theorem 3.**\n",
    "The Gauss-Seidel method converges if $A$ is Strictly Diagonally Dominant, for any initial approximation $x^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-allah",
   "metadata": {},
   "source": [
    "However, in practice the convergence rate as given by $C = C_{GS} = \\| (L + D)^{-1} U\\|$ is often better than for the\n",
    "$C = C_J = \\| D^{-1} (L+U) \\|$ for the Jacobi method.\n",
    "\n",
    "Sometimes this reduce the number of iterations enough to outweigh the extra computational effort involved in each iteration and make this faster overall than the Jacobi method — but not always."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-learning",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement and test the Gauss-Seidel method, and compare to Jacobi\n",
    "\n",
    "Do the two versions as above and use the same test cases.\n",
    "\n",
    "Then compare the speed/cost of the two methods:\n",
    "one way to do this is by using Python's \"stop watch\", function `time.time`: see the description of\n",
    "[Python module time](https://docs.python.org/3/library/time.html) in the Python manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-drink",
   "metadata": {},
   "source": [
    "## A family of test cases, arising from boundary value problems for differential equations\n",
    "\n",
    "The *tri-diagonal* matrices $T$ of the form\n",
    "\n",
    "$$\\begin{split}\n",
    "t_{i,i} &= 1 + 2 h^2\n",
    "\\\\\n",
    "t_{i,i+1} = t_{i,i+1} &= -h^2\n",
    "\\\\\n",
    "t_{i,j} &= 0, \\quad |i-j|> 1\n",
    "\\end{split}$$\n",
    "\n",
    "and variants of this arise in the solutions of boundary value problems for ODEs like\n",
    "\n",
    "$$\\begin{split}\n",
    "-u''(x) + K u &= f(x), \\quad a \\leq x \\leq b\n",
    "\\\\\n",
    "u(a) = u(b) &= 0\n",
    "\\end{split}$$\n",
    "\n",
    "and related problems for partial differential equations.\n",
    "\n",
    "Thus these provide useful initial test cases — usually with $h = (b-a)/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-retro",
   "metadata": {},
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
