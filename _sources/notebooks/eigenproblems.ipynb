{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Eigenvalues and Eigenvectors: the Power Method, and a bit beyond\n",
    "\n",
    "**Version of April 13, 2021**\n",
    "\n",
    "**Brenton LeMesurier**\n",
    "\n",
    "University of Northern Colorado, Greeley, Colorado in Spring 2021,\n",
    "[brenton.lemesurier@unco.edu](mailto:brenton.lemesurier@unco.edu);\n",
    "<br>\n",
    "More permanent address: The College of Charleston, Charleston, South Carolina,\n",
    "[lemesurierb@cofc.edu](mailto:lemesurierb@cofc.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "- Section 12.1 *Power Iteration Methods* of [Sauer](../references.html#Sauer)\n",
    "\n",
    "- Section 7.2 *Eigenvalues and Eigenvectors* of [Burden&Faires](../references.html#Burden-Faires).\n",
    "\n",
    "- Chapter 8, *More on Linear Equations* of [Chenney&Kincaid](../references.html#Chenney-Kincaid),\n",
    "in particular Section 3 *Power Method*, and also Section 2 *Eigenvalues and Eigenvectors* as background reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *eigenproblem* for a square $n \\times n$ matrix $A$ is to compute some or all non-trivial solutions of\n",
    "\n",
    "$$A \\vec{v} = \\lambda \\vec{v}.$$\n",
    "\n",
    "(By non-trivial, I mean to exclude $\\vec{v} = 0$, which gives a solution for any $\\lambda$.)\n",
    "That is, to compute\n",
    "the eigenvalues $\\lambda$ (of which generically there are $n$, but sometimes less)\n",
    "and the eigenvectors $\\vec{v}$ corresponding to each.\n",
    "\n",
    "With eigenproblems, and particularly those arising from differential equations, one often needs only the few smallest and/or largest eigenvalues. For these, the power method described below can be adapted, giving the shifted inverse power method.\n",
    "\n",
    "Here we often restirx our attention to the case of a real symmetric matrix ($A^T = A$, or $A_{ij} = A_{ji}$), or a Hermitian matrix ($A^T = A*$), for which many things are a bit simpler:\n",
    "- all eigenvalues are real,\n",
    "- for symmetric matrices, all eigenvectors are also real,\n",
    "- there is a complete set of orthogonal eigenvectors $\\vec{v}_k$, $1 \\leq i \\leq n$ that form a basis for all vectors\n",
    "and so on.\n",
    "\n",
    "However, the methods described here can be used more generally, or can be made to work with minor adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalue are roots of the characteristic polynomial, $\\det(A - \\lambda I)$;\n",
    "repeated roots are possible, and they will all be named, so there are always values $\\lambda_i$, $1 \\leq i \\leq n$.\n",
    "Here, these eigenvalues will be enumerated in decreasing order of magnitude:\n",
    "\n",
    "$$|\\lambda_1| \\geq |\\lambda_2| \\cdots \\geq |\\lambda_n|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power Method\n",
    "\n",
    "The basic tool is the *Power Method*, which will usually *but not always* succeed in computing the eigenvalue of largest magnitude, $\\lambda_1$, and **a** corresponding eigenvector $\\vec{v}_1$.\n",
    "Its success mainly involves assuming there being a unique largest eigenvalue: $\\lambda_1 > \\lambda_i$ for $i>1$.\n",
    "\n",
    "In its simplest form, one starts with a unit-length vector $\\vec{x}^{\\,0}$,\n",
    "so $\\|\\vec{x}^{\\,0}\\| = 1$, constructs the successive multiples\n",
    "$\\vec{y}^{\\,k} = A^k \\vec{x}^{\\,0}$\n",
    "by successive multiplications, and rescales at each stage to the unit vectors\n",
    "$\\vec{x}^{\\,k} = \\vec{y}^{\\,k}/\\|\\vec{y}^{\\,k}\\|$.\n",
    "\n",
    "Note that $\\vec{y}^{\\,k+1} = A \\vec{x}^{\\,k}$, so that once $\\vec{x}^{\\,k}$ is approximately an eigenvector for eigenvalue $\\lambda$, $\\vec{y}^{\\,k+1} \\approx \\lambda \\vec{x}^{\\,k}$, leading to the eigenvalue approximation\n",
    "\n",
    "$$r^{(k)} := \\langle\\vec{y}^{\\,k+1}, \\vec{x}^{\\,k} \\rangle\n",
    "\\approx \\langle\\lambda \\vec{x}^{\\,k}, \\vec{x}^{\\,k}\\rangle \\approx \\lambda$$\n",
    "\n",
    "**Note:** Here and below, I use $\\langle \\vec{a}, \\vec{b}\\rangle$ to denote the inner product (dot product) of two vectors; with Python/NumPy arrays this is given by `a.dot(b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic algorithm for the Power Method\n",
    "\n",
    "*Choose* initial vector $\\vec{y}_0$, maybe with a random number generator.\n",
    "\n",
    "Normalize to $\\vec{x}^{\\,0} = \\vec{y}^{\\,0}/\\|\\vec{y}^{\\,0}\\|$.\n",
    "\n",
    "for $k$ from 1 to $k_{max}$\n",
    "<br>\n",
    "$\\quad$ $\\vec{y}^{\\,k+1} = A \\vec{x}^{\\,k}$\n",
    "<br>\n",
    "$\\quad$ $r^{(k)} = \\langle \\vec{y}^{\\,k+1}, \\vec{x}^{\\,k} \\rangle$\n",
    "<br>\n",
    "$\\quad$ $\\vec{x}^{\\,k+1} = \\vec{y}^{\\,k+1}/\\|\\vec{y}^{\\,k+1}\\|$\n",
    "<br>\n",
    "end for\n",
    "\n",
    "The final values of $r^{(k)}$ and $\\vec{x}^{\\,k}$ approximate $\\lambda_1$ and $\\vec{v}_1$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement: deciding the iteration count\n",
    "\n",
    "Some details are omitted above; above all, how to decide the number of iterations.\n",
    "\n",
    "One approach is to use the fact that an eigenvector-eigenvalue pair satisfies\n",
    "$A \\vec{v} - \\lambda \\vec{v} = 0$, so the \"residual norm\"\n",
    "\n",
    "$$\n",
    "\\frac{\\|A \\vec{x}^{\\,k} - r^{(k)} \\vec{x}^{\\,k}\\|}{\\|\\vec{x}^{\\,k}\\|},\n",
    "= \\|\\vec{y}^{\\,k+1} - r^{(k)} \\vec{x}^{\\,k}\\| \\text{ since $\\|\\vec{x}^{\\,k}\\| = 1$}\n",
    "$$\n",
    "\n",
    "is a measure of \"relative backward error\".\n",
    "\n",
    "Thus one could repace the above `for` loop by a `while` loop based on a condition like stopping when\n",
    "\n",
    "$$\\|\\vec{y}^{\\,k+1} - r^{(k)} \\vec{x}^{\\,k}\\| \\leq \\epsilon.$$\n",
    "\n",
    "Alternatively, keep the `for` loop, but exit early (with `break`) if this condition is met.\n",
    "\n",
    "I generally recommend this `for-if-break` form for implementing iterative methods, because it makes avoidance of infinite loops simpler, and avoids the common `while` loop issue that you do not yet have an error estimate when the loop starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inverse Power Method\n",
    "\n",
    "The next step is to note that if $A$ is nonsingular, its inverse $B = A^{-1}$ has the same eigenvectors, but with eigenvalues $\\mu_i = 1/\\lambda_i$.\n",
    "\n",
    "Thus we can apply the power method to $B$ in order to compute its largest eigenvalue, which is $\\mu_n = 1/\\lambda_n$, along with the corresponding eigenvector $\\vec{v}_n$.\n",
    "\n",
    "The main change to the above is that\n",
    "\n",
    "$$\\vec{y}^{\\,k+1} = A^{-1} \\vec{x}_{k}.$$\n",
    "\n",
    "However, as usual one can (and should) avoid actually computing the inverse.\n",
    "Instead, express the above as the sysem of equations.\n",
    "\n",
    "$$A \\vec{y}^{\\,k+1} = \\vec{x}_{k}.$$\n",
    "\n",
    "Here is an important case where the LU factorization method can speed things up greatly: a single LU factorization is needed, after which for each $k$ one only has to do the far quicker forward and backward substitution steps: $O(n^2)$ cost for each iteration instead of $O(n^3/3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic algorithm for the Inverse Power Method\n",
    "\n",
    "*Choose* initial vector $\\vec{y}_0$, maybe with a random number generator.\n",
    "\n",
    "Normalize to $\\vec{x}^{\\,0} = \\vec{y}^{\\,0}/\\|\\vec{y}^{\\,0}\\|$.\n",
    "\n",
    "Compute an LU factorization $L U = A$.\n",
    "\n",
    "for $k$ from 1 to $k_{max}$\n",
    "<br>\n",
    "$\\quad$ Solve $L \\vec{z}^{\\,k+1} = \\vec{x}^{\\,k}$\n",
    "<br>\n",
    "$\\quad$ Solve $U \\vec{y}^{\\,k+1} = \\vec{z}^{\\,k+1}$\n",
    "<br>\n",
    "$\\quad$ $r^{(k)} = \\langle \\vec{y}^{\\,k+1}, \\vec{x}^{\\,k} \\rangle$\n",
    "<br>\n",
    "$\\quad$ $\\vec{x}^{\\,k+1} = \\vec{y}^{\\,k+1}/\\| \\vec{y}^{\\,k+1} \\|$\n",
    "<br>\n",
    "end for\n",
    "\n",
    "(If all goes well) the final values of $r^{(k)}$ and $\\vec{x}^{\\,k}$ approximate $\\lambda_n$ and $\\vec{v}_n$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting other eigenvalues with the Shifted Inverse Power Method\n",
    "\n",
    "The inverse power method computes the eigenvalue closest to 0; by *shifting*, we can compute the eigenvalue closest to any chosen value $s$.\n",
    "Then by searching various values of $s$, we can hope to find all the eigenvectors.\n",
    "As a variant, once we have $\\lambda_1$ and $\\lambda_n$, we can search nearby for other large or small eigenvalues: often the few largest and/or the few smallest are most important.\n",
    "\n",
    "With a symmetric (or Hermitian) matrix, once the eigenvalue of largest magnitude, $\\lambda_1$ is known, the rest are known to be real values in the interval $[-|\\lambda_1|,|\\lambda_1|]$, so we know roughly where to seek them.\n",
    "\n",
    "The main idea here is that for any number $s$, matrix $A - s I$ has eigenvalues\n",
    "$\\lambda_i  - s$, with the same eigenvectors as $A$:\n",
    "\n",
    "$$(A - sI)\\vec{v}_i = (\\lambda_i  - s)\\vec{v}_i$$\n",
    "\n",
    "Thus, applying the inverse power method to $A - s I$ computes its largest eigenvalue $\\gamma$,\n",
    "and then $\\lambda = 1/(\\gamma + s)$ is the eigenvalue of $A$ closest to $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further topics: getting all the eigenvalues with the QR Method, etc.\n",
    "\n",
    "The above methods are not ideal when many or all of the eigenvalues of a matrix are wanted; then a variety of more advanced methods have been developed, starting with the QR (factorization) Method.\n",
    "\n",
    "We will not address the details of that method in this course, but one way to think about it for a symmetric matrix is that:\n",
    "- The eigenvectors are orthogonal.\n",
    "\n",
    "- Thus, if after computing $\\lambda_1$ and $\\vec{v}_1$, one uses the power iteration starting with $\\vec{x}^{\\,0,2}$ orthogonal to $\\vec{v}_1$, then all the new iterates $\\vec{x}^{\\,k,2}$ will stay orthogonal, and one will get the eignevector corresponding to the largest remaining eigenvector: you get $\\vec{v}_2$ and $\\lambda_2$.\n",
    "\n",
    "- Continuing likewise, one can get the eigenvalues in descending order of magnitude.\n",
    "\n",
    "- As a modification, one can do all these almost in parallel: at iteration $k$, have an approximation $\\vec{x}^{\\,k,i}$ for each $\\lambda_i$ and at each stage, got by adjusting these new approximations so that $\\vec{x}^{\\,k,i}$ is orthogonal to all the approximations $\\vec{x}^{\\,k,j}$, $j < i$, for all the previous (larger) eigenvalues.\n",
    "This uses a variant of the *Gram-Schmidt* method for orthogonalizing a set of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
